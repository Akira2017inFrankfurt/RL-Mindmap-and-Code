{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddlepaddle        1.6.3\n",
      "paddlepaddle-gpu    1.8.3.post107\n",
      "parl                1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep paddlepaddle\n",
    "!pip list | grep parl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-20.2-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.1.1\n",
      "    Uninstalling pip-20.1.1:\n",
      "      Successfully uninstalled pip-20.1.1\n",
      "Successfully installed pip-20.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import paddle.fluid as fluid\n",
    "import parl\n",
    "from parl import layers\n",
    "from parl.utils import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(parl.Model):\n",
    "    def __init__(self, act_dim):\n",
    "        act_dim = act_dim\n",
    "        hid1_size = act_dim * 10\n",
    "\n",
    "        self.fc1 = layers.fc(size=hid1_size, act='tanh')\n",
    "        self.fc2 = layers.fc(size=act_dim, act='softmax')\n",
    "\n",
    "    def forward(self, obs):  # 可直接用 model = Model(5); model(obs)调用\n",
    "        out = self.fc1(obs)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient(parl.Algorithm):\n",
    "    def __init__(self, model, lr=None):\n",
    "        \"\"\" Policy Gradient algorithm\n",
    "        \n",
    "        Args:\n",
    "            model (parl.Model): policy的前向网络.\n",
    "            lr (float): 学习率.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "        assert isinstance(lr, float)\n",
    "        self.lr = lr\n",
    "\n",
    "    def predict(self, obs):\n",
    "        \"\"\" 使用policy model预测输出的动作概率\"\"\"\n",
    "        return self.model(obs)\n",
    "\n",
    "    def learn(self, obs, action, reward):\n",
    "        \"\"\" 用policy gradient 算法更新policy model \"\"\"\n",
    "        act_prob = self.model(obs)  # 获取输出动作概率\n",
    "        # log_prob = layers.cross_entropy(act_prob, action) # 交叉熵\n",
    "        log_prob = layers.reduce_sum(\n",
    "            -1.0 * layers.log(act_prob) * layers.one_hot(\n",
    "                action, act_prob.shape[1]),\n",
    "            dim=1)\n",
    "        cost = log_prob * reward\n",
    "        cost = layers.reduce_mean(cost)\n",
    "\n",
    "        optimizer = fluid.optimizer.Adam(self.lr)\n",
    "        optimizer.minimize(cost)\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(parl.Agent):\n",
    "    def __init__(self, algorithm, obs_dim, act_dim):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(Agent, self).__init__(algorithm)\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):  # 搭建计算图用于 预测动作，定义输入输出变量\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.act_prob = self.alg.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(\n",
    "                self.learn_program):  # 搭建计算图用于 更新policy网络，定义输入输出变量\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            act = layers.data(name='act', shape=[1], dtype='int64')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            self.cost = self.alg.learn(obs, act, reward)\n",
    "\n",
    "    def sample(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)  # 增加一维维度\n",
    "        act_prob = self.fluid_executor.run(\n",
    "            self.pred_program,\n",
    "            feed={'obs': obs.astype('float32')},\n",
    "            fetch_list=[self.act_prob])[0]\n",
    "        act_prob = np.squeeze(act_prob, axis=0)  # 减少一维维度\n",
    "        act = np.random.choice(range(self.act_dim), p=act_prob)  # 根据动作概率选取动作\n",
    "        return act\n",
    "\n",
    "    def predict(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act_prob = self.fluid_executor.run(\n",
    "            self.pred_program,\n",
    "            feed={'obs': obs.astype('float32')},\n",
    "            fetch_list=[self.act_prob])[0]\n",
    "        act_prob = np.squeeze(act_prob, axis=0)\n",
    "        act = np.argmax(act_prob)  # 根据动作概率选择概率最高的动作\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward):\n",
    "        act = np.expand_dims(act, axis=-1)\n",
    "        feed = {\n",
    "            'obs': obs.astype('float32'),\n",
    "            'act': act.astype('int64'),\n",
    "            'reward': reward.astype('float32')\n",
    "        }\n",
    "        cost = self.fluid_executor.run(\n",
    "            self.learn_program, feed=feed, fetch_list=[self.cost])[0]\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent):\n",
    "    obs_list, action_list, reward_list = [], [], []\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        obs_list.append(obs)\n",
    "        action = agent.sample(obs) # 采样动作\n",
    "        action_list.append(action)\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        reward_list.append(reward)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return obs_list, action_list, reward_list\n",
    "\n",
    "# 评估 agent, 跑 5 个episode，总reward求平均\n",
    "def evaluate(env, agent, render=False):\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            action = agent.predict(obs) # 选取最优动作\n",
    "            obs, reward, isOver, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if isOver:\n",
    "                break\n",
    "        eval_reward.append(episode_reward)\n",
    "    return np.mean(eval_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08-03 16:30:58 MainThread @<ipython-input-11-6b5a2e549c1a>:13]\u001b[0m obs_dim 4, act_dim 2\n",
      "\u001b[32m[08-03 16:30:58 MainThread @machine_info.py:86]\u001b[0m nvidia-smi -L found gpu count: 1\n",
      "\u001b[32m[08-03 16:30:58 MainThread @machine_info.py:86]\u001b[0m nvidia-smi -L found gpu count: 1\n",
      "\u001b[32m[08-03 16:30:58 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 0, Reward Sum 10.0.\n",
      "\u001b[32m[08-03 16:30:59 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 10, Reward Sum 19.0.\n",
      "\u001b[32m[08-03 16:30:59 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 20, Reward Sum 28.0.\n",
      "\u001b[32m[08-03 16:30:59 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 30, Reward Sum 9.0.\n",
      "\u001b[32m[08-03 16:30:59 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 40, Reward Sum 14.0.\n",
      "\u001b[32m[08-03 16:30:59 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 50, Reward Sum 27.0.\n",
      "\u001b[32m[08-03 16:31:00 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 60, Reward Sum 29.0.\n",
      "\u001b[32m[08-03 16:31:00 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 70, Reward Sum 11.0.\n",
      "\u001b[32m[08-03 16:31:00 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 80, Reward Sum 26.0.\n",
      "\u001b[32m[08-03 16:31:00 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 90, Reward Sum 20.0.\n",
      "\u001b[32m[08-03 16:31:01 MainThread @<ipython-input-11-6b5a2e549c1a>:39]\u001b[0m Test reward: 9.2\n",
      "\u001b[32m[08-03 16:31:01 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 100, Reward Sum 14.0.\n",
      "\u001b[32m[08-03 16:31:01 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 110, Reward Sum 32.0.\n",
      "\u001b[32m[08-03 16:31:02 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 120, Reward Sum 23.0.\n",
      "\u001b[32m[08-03 16:31:02 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 130, Reward Sum 38.0.\n",
      "\u001b[32m[08-03 16:31:02 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 140, Reward Sum 36.0.\n",
      "\u001b[32m[08-03 16:31:03 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 150, Reward Sum 21.0.\n",
      "\u001b[32m[08-03 16:31:03 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 160, Reward Sum 19.0.\n",
      "\u001b[32m[08-03 16:31:03 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 170, Reward Sum 24.0.\n",
      "\u001b[32m[08-03 16:31:04 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 180, Reward Sum 36.0.\n",
      "\u001b[32m[08-03 16:31:04 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 190, Reward Sum 44.0.\n",
      "\u001b[32m[08-03 16:31:12 MainThread @<ipython-input-11-6b5a2e549c1a>:39]\u001b[0m Test reward: 112.2\n",
      "\u001b[32m[08-03 16:31:12 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 200, Reward Sum 48.0.\n",
      "\u001b[32m[08-03 16:31:12 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 210, Reward Sum 33.0.\n",
      "\u001b[32m[08-03 16:31:13 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 220, Reward Sum 80.0.\n",
      "\u001b[32m[08-03 16:31:13 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 230, Reward Sum 27.0.\n",
      "\u001b[32m[08-03 16:31:14 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 240, Reward Sum 11.0.\n",
      "\u001b[32m[08-03 16:31:15 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 250, Reward Sum 43.0.\n",
      "\u001b[32m[08-03 16:31:15 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 260, Reward Sum 34.0.\n",
      "\u001b[32m[08-03 16:31:16 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 270, Reward Sum 41.0.\n",
      "\u001b[32m[08-03 16:31:16 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 280, Reward Sum 92.0.\n",
      "\u001b[32m[08-03 16:31:17 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 290, Reward Sum 105.0.\n",
      "\u001b[32m[08-03 16:31:31 MainThread @<ipython-input-11-6b5a2e549c1a>:39]\u001b[0m Test reward: 197.4\n",
      "\u001b[32m[08-03 16:31:31 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 300, Reward Sum 36.0.\n",
      "\u001b[32m[08-03 16:31:32 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 310, Reward Sum 51.0.\n",
      "\u001b[32m[08-03 16:31:33 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 320, Reward Sum 98.0.\n",
      "\u001b[32m[08-03 16:31:34 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 330, Reward Sum 51.0.\n",
      "\u001b[32m[08-03 16:31:36 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 340, Reward Sum 95.0.\n",
      "\u001b[32m[08-03 16:31:37 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 350, Reward Sum 35.0.\n",
      "\u001b[32m[08-03 16:31:38 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 360, Reward Sum 94.0.\n",
      "\u001b[32m[08-03 16:31:38 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 370, Reward Sum 21.0.\n",
      "\u001b[32m[08-03 16:31:40 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 380, Reward Sum 110.0.\n",
      "\u001b[32m[08-03 16:31:41 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 390, Reward Sum 56.0.\n",
      "\u001b[32m[08-03 16:31:54 MainThread @<ipython-input-11-6b5a2e549c1a>:39]\u001b[0m Test reward: 182.4\n",
      "\u001b[32m[08-03 16:31:54 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 400, Reward Sum 122.0.\n",
      "\u001b[32m[08-03 16:31:56 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 410, Reward Sum 131.0.\n",
      "\u001b[32m[08-03 16:31:57 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 420, Reward Sum 95.0.\n",
      "\u001b[32m[08-03 16:31:58 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 430, Reward Sum 140.0.\n",
      "\u001b[32m[08-03 16:32:00 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 440, Reward Sum 162.0.\n",
      "\u001b[32m[08-03 16:32:01 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 450, Reward Sum 118.0.\n",
      "\u001b[32m[08-03 16:32:03 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 460, Reward Sum 152.0.\n",
      "\u001b[32m[08-03 16:32:04 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 470, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:32:06 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 480, Reward Sum 141.0.\n",
      "\u001b[32m[08-03 16:32:07 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 490, Reward Sum 90.0.\n",
      "\u001b[32m[08-03 16:32:21 MainThread @<ipython-input-11-6b5a2e549c1a>:39]\u001b[0m Test reward: 198.4\n",
      "\u001b[32m[08-03 16:32:22 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 500, Reward Sum 136.0.\n",
      "\u001b[32m[08-03 16:32:23 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 510, Reward Sum 126.0.\n",
      "\u001b[32m[08-03 16:32:25 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 520, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:32:26 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 530, Reward Sum 146.0.\n",
      "\u001b[32m[08-03 16:32:27 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 540, Reward Sum 149.0.\n",
      "\u001b[32m[08-03 16:32:29 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 550, Reward Sum 51.0.\n",
      "\u001b[32m[08-03 16:32:31 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 560, Reward Sum 169.0.\n",
      "\u001b[32m[08-03 16:32:32 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 570, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:32:34 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 580, Reward Sum 148.0.\n",
      "\u001b[32m[08-03 16:32:36 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 590, Reward Sum 147.0.\n",
      "\u001b[32m[08-03 16:32:50 MainThread @<ipython-input-11-6b5a2e549c1a>:39]\u001b[0m Test reward: 198.4\n",
      "\u001b[32m[08-03 16:32:51 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 600, Reward Sum 148.0.\n",
      "\u001b[32m[08-03 16:32:52 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 610, Reward Sum 182.0.\n",
      "\u001b[32m[08-03 16:32:54 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 620, Reward Sum 150.0.\n",
      "\u001b[32m[08-03 16:32:55 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 630, Reward Sum 106.0.\n",
      "\u001b[32m[08-03 16:32:57 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 640, Reward Sum 144.0.\n",
      "\u001b[32m[08-03 16:32:59 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 650, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:33:00 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 660, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:33:02 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 670, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:33:04 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 680, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:33:05 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 690, Reward Sum 191.0.\n",
      "\u001b[32m[08-03 16:33:21 MainThread @<ipython-input-11-6b5a2e549c1a>:39]\u001b[0m Test reward: 200.0\n",
      "\u001b[32m[08-03 16:33:21 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 700, Reward Sum 200.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08-03 16:33:23 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 710, Reward Sum 168.0.\n",
      "\u001b[32m[08-03 16:33:25 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 720, Reward Sum 146.0.\n",
      "\u001b[32m[08-03 16:33:27 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 730, Reward Sum 125.0.\n",
      "\u001b[32m[08-03 16:33:29 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 740, Reward Sum 120.0.\n",
      "\u001b[32m[08-03 16:33:31 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 750, Reward Sum 185.0.\n",
      "\u001b[32m[08-03 16:33:32 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 760, Reward Sum 144.0.\n",
      "\u001b[32m[08-03 16:33:34 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 770, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:33:36 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 780, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:33:38 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 790, Reward Sum 139.0.\n",
      "\u001b[32m[08-03 16:33:54 MainThread @<ipython-input-11-6b5a2e549c1a>:39]\u001b[0m Test reward: 200.0\n",
      "\u001b[32m[08-03 16:33:54 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 800, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:33:56 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 810, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:33:58 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 820, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:34:00 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 830, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:34:02 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 840, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:34:03 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 850, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:34:05 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 860, Reward Sum 146.0.\n",
      "\u001b[32m[08-03 16:34:07 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 870, Reward Sum 167.0.\n",
      "\u001b[32m[08-03 16:34:09 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 880, Reward Sum 149.0.\n",
      "\u001b[32m[08-03 16:34:11 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 890, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:34:26 MainThread @<ipython-input-11-6b5a2e549c1a>:39]\u001b[0m Test reward: 200.0\n",
      "\u001b[32m[08-03 16:34:26 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 900, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:34:28 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 910, Reward Sum 149.0.\n",
      "\u001b[32m[08-03 16:34:30 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 920, Reward Sum 180.0.\n",
      "\u001b[32m[08-03 16:34:32 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 930, Reward Sum 123.0.\n",
      "\u001b[32m[08-03 16:34:34 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 940, Reward Sum 132.0.\n",
      "\u001b[32m[08-03 16:34:36 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 950, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:34:38 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 960, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:34:39 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 970, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:34:41 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 980, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:34:43 MainThread @<ipython-input-11-6b5a2e549c1a>:30]\u001b[0m Episode 990, Reward Sum 200.0.\n",
      "\u001b[32m[08-03 16:34:58 MainThread @<ipython-input-11-6b5a2e549c1a>:39]\u001b[0m Test reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "# 根据一个episode的每个step的reward列表，计算每一个Step的Gt\n",
    "def calc_reward_to_go(reward_list, gamma=1.0):\n",
    "    for i in range(len(reward_list) - 2, -1, -1):\n",
    "        # G_t = r_t + γ·r_t+1 + ... = r_t + γ·G_t+1\n",
    "        reward_list[i] += gamma * reward_list[i + 1]  # Gt\n",
    "    return np.array(reward_list)\n",
    "\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make('CartPole-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n\n",
    "logger.info('obs_dim {}, act_dim {}'.format(obs_dim, act_dim))\n",
    "\n",
    "# 根据parl框架构建agent\n",
    "model = Model(act_dim=act_dim)\n",
    "alg = PolicyGradient(model, lr=LEARNING_RATE)\n",
    "agent = Agent(alg, obs_dim=obs_dim, act_dim=act_dim)\n",
    "\n",
    "# 加载模型\n",
    "# if os.path.exists('./model.ckpt'):\n",
    "#     agent.restore('./model.ckpt')\n",
    "#     run_episode(env, agent, train_or_test='test', render=True)\n",
    "#     exit()\n",
    "\n",
    "for i in range(1000):\n",
    "    obs_list, action_list, reward_list = run_episode(env, agent)\n",
    "    if i % 10 == 0:\n",
    "        logger.info(\"Episode {}, Reward Sum {}.\".format(\n",
    "            i, sum(reward_list)))\n",
    "\n",
    "    batch_obs = np.array(obs_list)\n",
    "    batch_action = np.array(action_list)\n",
    "    batch_reward = calc_reward_to_go(reward_list)\n",
    "\n",
    "    agent.learn(batch_obs, batch_action, batch_reward)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        total_reward = evaluate(env, agent, render=False) # render=True 查看渲染效果，需要在本地运行，AIStudio无法显示\n",
    "        logger.info('Test reward: {}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
